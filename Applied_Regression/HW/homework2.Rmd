---
title: "Stat 230 HW 2"
author: 'Name: Teagan Johnson'
output:
  pdf_document: default
  html_document: default
---
```{r, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, 
                      prompt=FALSE, 
                      message = FALSE, warning = FALSE)
```

### worked with: Parker Johnson and Miles Tisserand

Homework 2 is due **by noon, Tues 9/28**. Please complete the assignment in this Markdown document, filling in your answers and R code below.  I didn't create answer and R chunk fields like I did with homework 1, but please fill in your answers and R code in the same manner as hw 1. Make sure to follow the homework guidelines when writing up this assignment (handout is located on the right side of moodle page).

Tips for using Markdown with homework sets:

- Work through a problem by putting your R code into R chunks in this .Rmd. Run the R code to make sure it works, then knit the .Rmd to verify they work in that environment. 
- Make sure you load your data in the .Rmd and include any needed `library` commands. 

- Feel free to edit or delete  questions, instructions, or code provided in this file when producing your homework solution. 

- For your final document, you can change the output type from `html_document` to `word_document` or `pdf_document`. These two to output types are better formatted for printing. 

- Keep the hw problems in **problem order** I give in this doc. You can attach hand-written work for a problem (if needed) but make it clear in this doc when you are answering a problem using work attached to your printed pdf/word doc. 

-----------------------------------

## Problem 1: Big Bang:  ch. 7 exercise 13

Comment: Complete this problem "by hand" using the info in Display 7.9 (i.e. *don't* load the data and fit a `lm`). Use the `qt` command in R to get your mutliplier $t^*$ for the CI calculation.

*Answer: * 

qt(.975, 22) = 2.073873 = $t^*$

.3991 $\pm$ (2.073873) * (.1185)

.3991 $\pm$ (2.4575)

The 95% confidence interval is (.1533, .63449) for the intercept of the regression line.

```{r}
qt(.975, 22)
```

-----------------------------------

## Problem 2: Agstrat data revisited
Recall the `agstrat.csv` data used for homework 1. This was a stratified random sample of US counties. We will consider the regression of farm acreage in 1992 (y=`acres92`) on farm acreage in 1982 (x=`acres82`).

#### (2a)  Use the `ggplot2` package to create a scatterplot of `acres92` against `acres82` that includes the fitted regression line. Describe the direction, form and strenth of the relationship shown in this graph.

*Answer: *

The direction is positive, the form is linear, and the strength is relatively strong. There is, however, more variance in the points (and overall less points) further along the graph.

```{r}
library(ggplot2)
agstrat <- read.csv("http://people.carleton.edu/~kstclair/data/agstrat.csv")

lm <- lm(acres92 ~ acres82, data = agstrat)

ggplot(agstrat, aes(x=acres82, y=acres92)) + labs(x="farm acres per county in 1982", y="farm acres per county in 1992", title="Acres in 1992 vs acres in 1982") + geom_point() + geom_smooth(method="lm")
```

#### (2b) Fit the regression of `acres92` on `acres82`. Interpret the slope in context for this problem. 

*Answer: *

The slope in the regression of acres92 vs. acres 82 is .984. This means that an increase of 1 farm acre per county in 1982 generally corresponds to an increase of just under 1 farm acre per county in 1992.

```{r}
library(broom)
tidy(lm)
```

#### (2c)  Compute the test statistic and p-value to test $H_0: \beta_1 = 0$ vs $H_A: \beta_1 \neq 0$ using the estimated slope and SE from part (b). Your answers should match the values given in part (b), but I want to see your *work* showing how these values are calculated in part (c). Then *interpret* the test stat and p-value in context. 

*Answer: *

The test statistic is 139.92 and the p-value is basically 0 (p=7.47271e-274). With a value of 139.92 and assuming the null hypothesis is true, the test statistic means that the observed results are 139.92 standard deviations away from $\hat{B}$ = 0 (the mean assuming the null hypothesis is true). The p-value, 7.47271e-274, means that there was an extremely low probability of our observed regression line slope (.984) assuming the null hypothesis was true.

The p-value provides statistically significant evidence to disprove the null hypothesis.

*Work for t statistic*

t = ($\hat{B}$ - 0) / (7.035297e-03)

t = 139.923

*Work for p-value*

I used the pt command in R and got 7.47271e-274

```{r}
2*pt(-139.923, 300-2)
```

#### (2d) Are the four regression model assumptions satistified for your model in part (b) (e.g. linearity, constant variance, normality and independence). Make sure to show and explain all graphs used to assess these assumptions. 

*Answer: *

Looking at the residual plot, the linearity assumption is satisfied. There is no clear curvature amongst the points and there is similar scatter above+below 0.

The constant variance assumption is not satisfied. There are many more points at the start of the residual plot and very few as the x value increases.

The normality assumption is not satisfied. Looking at the qq-plot, the tails are not closely related to the straight line. The tails indicate that the histogram of residuals has fat tails, therefore not satisfying the normality assumption.

The independence assumption is not satisfied. The two groups of points are spacially related: if there are x amount of farms per county in 1982, that heavily influences how many farms per county there are in 1992.

```{r}
library(ggResidpanel)
farmdata_aug <- augment(lm)
ggplot(farmdata_aug, aes(x=acres82, y=.resid)) + geom_point() + geom_hline(yintercept=0, linetype = "dashed") + labs(y = "residuals", x = "farm acres per country in 1982", title = "Residual Plot of farm data")

hist(farmdata_aug$.resid)
resid_panel(lm, plots="qq")
```

-----------------------------------


## Problem 3: Meat Processing:  ch. 7 exercises 17 (a-b only), 18, 20

#### 17(a) Enter the data from Display 7.3 into a computer and find the least squares estimates for the simple linear regression of pH on log hours. 

*Answer: *

The estimate of pH on log hours is -.726. This means that for an increase of 1 log(Time), there is a .726 decrease in the pH of the carcasses.

The estimate of the intercept is 6.984. This means that when log(Time) is 0, the pH is 6.984.
```{r}
library(Sleuth3)

```

```{r}
meatProclm <- lm(pH ~ log(Time), data=case0702)
tidy(meatProclm)
```

- For 17(a), the data for this example is called `case0702` which is in the `Sleuth3` package. Your R chunk should look like
```{r}
library(Sleuth3)

```

#### 17(b) Noting that the average and sample standard deviation of X are provided in Display 7.12, calculate the estimated mean pH at 5 hours after slaughter, and the standard error of the estimated mean (using the formula in Section 7.4.2).

*Answer: *

The estimated mean pH at 5 hours after slaughter is 6.98 + (ln(5)*-.73) = 5.816. The predict function also gave us 5.816. The standard error, using the se.fit and residual.scale values along with plugging the values into the equation manually, is $\sqrt{((0.02975^2)+(0.08226^2))}=0.0875$

```{r}
predict(meatProclm, data.frame(Time=5), interval = "prediction", se.fit = TRUE)
```

#### 18(a) Find the standard error of prediction for the prediction of pH at 5 hours after slaughter.

*Answer: *

The standard error of prediction for the prediction of pH at 5 hours after slaughter is 0.02975. This is se.fit value we got from 17(b).

#### 18(b) Construct a 95% prediction interval at 5 hours after slaughter.

*Answer: *

Looking at our predict function from 17(b), our 95% confidence interval 5 hours after slaughter is (5.614009, 6.01744).

#### 20 The standard error of the estimated slope based on the 10 data points is 0:0344. Using the formula for SE in Section 7.3.5, and supposing that the spread of the Xâ€™s and the estimate of will be about the same in a future study, calculate how large the sample size would have to be in order for the SE of the estimated slope to be 0:01.

*Answer: *

$SE(\hat{\beta_1}) = \hat{\sigma}\sqrt{\frac{1}{(n-1)s_x^2}}$

The sample size would have to be at least 108. Using the formula above, we plugged in .01 for $SE(\hat{\beta_1})$, .0822 for $\hat{\sigma}$, and .634 for $s_x^2$. Then we solved for n.

```{r}
1/(((.01/.0822)^2)*.634) + 1
var(log(case0702$Time))
```

- Make sure that you use `log(TIME)` as the predictor in your model: `lm(pH ~ log(Time), data=case0702)`
- You don't need to do 17(b) by hand, just use the `predict` command in R. Important note: If you log a predictor in your `lm` command, e.g. `lm(y ~ log(x))`, then you give the `predict` command the value of `x` on the **original** (unlogged) scale when entering a value for `newdata`.

-----------------------------------


## Problem 4: Biological Pest Control: ch. 8 exercise 17

#### 17(a) Use scatterplots of the raw data, along with trial and error, to determine transformations of Y D Ragwort dry mass and of X D Flea beetle load that will produce an approximate linear relationship.

*Answer: *

If you take the log of the Load, the x value, and the square root of the Mass, the y value, the scatterplot of Load vs. Mass is relatively linear. After comparing all 4 of the different combinations of log and sqrt, this one seemed to be the most linear.

```{r}
head(ex0817)
```
```{r}
ggplot(data=ex0817, aes(x=Load, y=Mass)) + geom_point() + scale_y_sqrt() + scale_x_log10() +  geom_smooth(method="lm") + labs(x="log(Load)", y="log(Mass)", title="Flea beetle load vs. Ragwort Mass")
```

#### 17(b) Fit a linear regression model on the transformed scale; calculate residuals and fitted values.

*Answer: *

The fitted values: $y_i = 6.6197337	- 0.9626561x_i$ where 6.6197337 is $B_0$ and -0.9626561 is $B_1$. Below is a residual plot using the data fitted to the linear model.

```{r}
pest_lm <- lm(sqrt(Mass) ~ log(Load), ex0817)
resid_xpanel(pest_lm)

pest_lm_aug <- augment(pest_lm)
head(pest_lm_aug)

tidy(pest_lm)
```

#### 17(c) Look at the residual plot. Do you want to try other transformations? What do you suggest?

*Answer: *

It would be interesting to see how multiple regression transformations would work with this data. Would the data be better fitted? If I were to suggest something, it would be exactly what we learned in class: perform trial and error with different transformations (multiple regression, time series analysis transformations, reciprocal transformations, etc.) until you find one that best fits the data.

- Data is in the `Sleuth3` package (`ex0817`).
- For part (a), limit yourself to exploring logarithm and square root transformation for one or both variables. How can you make these graphs?
    - The `ggplot2` package let's you easily do this **without** applying those functions to your variables, instead you add another layer that tells R how to scale a particular axis. This method of visualization is nice because your numerical labels on the x/y axis will still be measured in the original units of the variables.  If you want to, say, look at the scatterplot of `sqrt(y)`  against `log10(x)` (base-10 log) you would add the layers `scale_y_sqrt()` and `scale_x_log10()` to your scatterplot of `y` against `x`.  For this example, that would look like:
```{r, eval = FALSE}
ggplot(pest, aes(x= Load, y = Mass)) + 
  geom_point() + 
  scale_y_sqrt() + 
  scale_x_log10()
```
- For part (b), fit the model and give the fitted regression equation.

-----------------------------------

## Problem 5: Pollen Removal ch. 8 exercise 19

#### 19(a) What problems are evident in the residual plot?

*Answer: *

The biggest problems I see with the residual plot is that there are a few extreme outliers. These could significantly affect the data by skewing the regression line. Also, to a lesser extent, the data points to the left of the residual plot are frequently below zero while the points to the right of the graph are frequently above 0. This could lead one to question the constant variance assumption.

```{r}
library(dplyr)
queen_bees <- filter(ex0327, BeeType == "Queen")
bees_lm <- lm(PollenRemoved ~ DurationOfVisit, queen_bees)
bees_lm_aug <- augment(bees_lm)
head(bees_lm_aug)
ggplot(data=bees_lm_aug, aes(x=DurationOfVisit, y=.resid)) + geom_point() + geom_hline(yintercept=0, linetype = "dashed")
```

#### 19(b) Do log transformations of Y or X help any?

*Answer: * None of the log transformations seem to help the data when looking at the scatter plots and residual plots, although taking the lof of the duration of visit was the closest to linear of the three transformations. We're still dealing with the same issues of extreme outliers and to a lesser extent the lack of constant variance.

```{r}
ggplot(queen_bees, aes(x=DurationOfVisit, y=PollenRemoved)) + geom_point() + geom_smooth(method="lm") + labs(title="scatterplot of duration vs. pollen removed")

ggplot(queen_bees, aes(x=DurationOfVisit, y=PollenRemoved)) + geom_point() + geom_smooth(method="lm") + labs(title="scatterplot of log(duration) vs. pollen removed") + scale_x_log10()

ggplot(queen_bees, aes(x=DurationOfVisit, y=PollenRemoved)) + geom_point() + geom_smooth(method="lm") + labs(title="scatterplot of duration vs. log(pollen removed)") + scale_y_log10()

ggplot(queen_bees, aes(x=DurationOfVisit, y=PollenRemoved)) + geom_point() + geom_smooth(method="lm") + labs(title="scatterplot of log(duration) vs. log(pollen removed)") + scale_x_log10() + scale_y_log10()

bees_lm_logy = lm(log(PollenRemoved) ~ DurationOfVisit, queen_bees)
resid_xpanel(bees_lm_logy)

bees_lm_logx = lm(PollenRemoved ~ log(DurationOfVisit), queen_bees)
resid_xpanel(bees_lm_logx)

bees_lm_logboth = lm(log(PollenRemoved) ~ log(DurationOfVisit), queen_bees)
resid_xpanel(bees_lm_logboth)
```

#### 19(c) Try fitting the regression only for those times less than 31 seconds (i.e., excluding the two longest times). Does this fit better?

*Answer: * Getting rid of the outliers seemed to help the data fit much better. There is less potential for individual points to impact the data, and the constant variance issues seen above have even seemed to decrease in significance. The data looks relatively linear when the outliers are taken out.

```{r}
fast_bees <- filter(queen_bees, DurationOfVisit < 31)
bees_lm_fast = lm(PollenRemoved ~ DurationOfVisit, fast_bees)
resid_xpanel(bees_lm_fast)
```

- The data for this problem is `ex0327`
- For part (b) scatterplot, use the `scale` layers described in Problem 4 hints.
- For part (c), use the `filter` command from `dplyr` to make a version of the data set that only contains cases where `DurationOfVisit < 31`



-------------------------------

## Problem 6: Normality Assumption
The (hidden) R chunk below defines a function named `reg.sim2` that samples responses from a  SLR model given a vector of explanatory values $x$. You will use the following SLR model to generate your set of responses:
$$
Y_{i} = 20 + 1x_{i} + \epsilon_{i} \ \ \ \ \epsilon_{i} \sim N(0,2)
$$
so that $\beta_{0}=20, \beta_{1}=1$ and $\sigma=2$. 


```{r, include=FALSE}
# R code for a function that simulates data from a given regression model
# input: 
# x= explanatory variable
# n= sample size
# beta0, beta1, sigma = model parameter values
# grph = T/F = plot the sampled values, data and residuals?
reg.sim2 <- function(x,beta0,beta1,sigma, grph=T)
{
  n<- length(x)
  muy.x <- beta0 + beta1 * x
  
  # simulate some y's:
  y <- rnorm(n, muy.x,sigma)
  
  mylm<- lm(y~x)
  
  # plot data, LS line, true line
  if (grph){
    par(mfrow=c(3,2))
    plot(x,y,xlab="x",ylab="simulated y",pch=20,ylim=c(beta0+beta1*min(x)-3, beta0+beta1*max(x)+3))
    points(mean(x),mean(y),pch="X",cex=2)
    abline(mylm,lwd=3,col="blue")
    abline(beta0,beta1,lwd=3,lty=2,col="red")
    legend("topleft",legend=c("Pop.","Samp."),col=c("red","blue"),lty=c(2,1))
    
    
   # hist(x); 
    plot(fitted(mylm),resid(mylm),main="residual plot"); abline(h=0)
    hist(y)
    qqnorm(y,main="QQ plot of y"); qqline(y)
    
    hist(resid(mylm)); qqnorm(resid(mylm),main="QQ plot of residuals"); qqline(resid(mylm))
    par(mfrow=c(1,1))
  }
  return(list(b0=mylm$coefficients[[1]], b1=mylm$coefficients[[2]], SE.b0=sqrt(vcov(mylm)[1,1]), SE.b1 =  sqrt(vcov(mylm)[2,2])))
}

```

#### 6a
We start by generating a sample of 1000 explanatory variable values. Suppose the explanatory variable $x$ is equally (uniformly) distribution between 1 and 10. Generate $x$  and view its distribution (use whatever seed value you like):

```{r, fig.width=4,fig.height=3}
set.seed(7)
x <- seq(from=1,to=10,length=1000)
hist(x)
```

Next, for each of the 1000  $x_i$'s that you just created, use the `reg.sim2` function to  generate 1000 responses $y_i$ from the population model described at the start of this problem:

```{r, fig.height=6}
reg.sim2(x, beta0=20, beta1=1, sigma=2, grph=T)
```

The R output gives the estimated slope and intercept ($\hat{\beta}_{1}, \hat{\beta}_{0}$), a scatterplot of the data (along with the sample and population regression lines), and histograms/QQnormal plots for the responses $y_i$ and the residuals $r_{i}$. Use the **histograms** and **QQ normal plots** to answer the following questions:


**Are the sampled $y$s normally distributed? If not, describe the general shape of their distribution.**

The sampled ys are mostly normally distributed, but the tails indicate that the ys are not completely normally distributed. Looking at the qq-plot, and also the histogram, it appears that the ys have thin tails (this is the general shape of their distribution). Although the qq plot indicates the ys are mostly normally distributed, the qq-plot tails are slightly curved, signifying the ys minor lack of normalcy.

**Are the residuals normally distributed? If not, describe the general shape of their distribution.**

The residuals appear to be normally distributed. The qq plot's points mostly lie on the straight qq-plot line, and the histogram appears to be relatively normal.

#### 6b Repeat part (a), but this time generate a sample of 1000 $x$'s that are skewed right using the command `rgamma(1000, 1, 1/2)`.

The ys are not normally distributed, the qq plot graph is concave up signifying a right skew. The residuals are still relatively normally distributed as the points on the qq plot are still close to the straight line.

```{r, fig.width=4,fig.height=3}
set.seed(7)
xgamma <- rgamma(1000, 1, 1/2)
hist(xgamma)
```
```{r}
reg.sim2(xgamma, beta0=20, beta1=1, sigma=2, grph=T)
```

#### 6c  Repeat part (a), but this time generate a sample of 1000 $x$'s that are normally distributed using the command `rnorm(1000, 10, 2)`.

Both the ys and the residuals are normally distributed. The data points on each qq plot tightly hug the straight line and the histograms look relatively normal.

```{r, fig.width=4,fig.height=3}
set.seed(7)
xnorm <- rnorm(1000, 10, 2)
hist(xnorm)
```
```{r}
reg.sim2(xnorm, beta0=20, beta1=1, sigma=2, grph=T)
```

#### 6d Use your simulation results from (a)-(c) to explain how the distribution of the explanatory variable $x$ can affect the distribution of the response.

The distribution of the explanatory variable x can affect the distribution of the response by making the ys skewed. If the xs are skewed one way, the response is skewed the same way. However, the distribution of the explanatory variable does not seem to have an effect on the distribution of the residuals. All 3 residual plots were normally distributed even when the distribution of x was not normal. This indicates that as long as the sample size is large enough, the residuals will be normally distributed. Ultimately, neither explanatory or response variables need to be normally distributed for the normality assumption to hold.

#### 6e (not  a question!) Moral: All the data generated for this problem satisfy the SLR model assumptions. Your take away from Q7 should be to see that *neither* your response nor  explanatory variables need to be *normally distributed* for the SLR model assumptions to hold. Rather, the SLR model says that the model errors (variation around the line) are normally distributed. Assessing the SLR "normality" assumption should focus on checking the distribution of the *residuals* rather than the distribution of the responses. 
