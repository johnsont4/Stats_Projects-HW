---
title: "Stat 230 HW 8"
author: "Name: Teagan Johnson"
output: pdf_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE, comment=NULL, message  = FALSE, warning = FALSE)
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)
library(gridExtra)

1-pchisq(34.92, 11)
```

Homework 8 is due **by 3pm Thursday, Nov 18**. Please complete the assignment in this Markdown document, filling in your answers and R code below.  I didn't create answer and R chunk fields like I did with homework 1, but please fill in your answers and R code in the same manner as hw 1.  Submit  a hard copy of the **compiled pdf or word doc** either

  - in class 
  - in drop-in office hours 
  - in the paper holder outside my CMC 222 office door

Tips for using Markdown with homework sets:

- Work through a problem by putting your R code into R chunks in this .Rmd. Run the R code to make sure it works, then knit the .Rmd to verify they work in that environment.
    - Make sure you load your data in the .Rmd and include any needed `library` commands. 
- Feel free to edit or delete  questions, instructions, or code provided in this file when producing your homework solution. 
- For your final document, you can change the output type from `html_document` to `word_document` or `pdf_document`. These two to output types are better formatted for printing. 
  - on maize: you may need to allow for pop-ups from this site 
- If you want to knit to pdf while running Rstudio from your computer (*not* from maize), you  will need a LaTeX compiler installed on your computer. This could be [MiKTeX](https://miktex.org/), [MacTeX](http://www.tug.org/mactex/) (mac), or TinyTex. The latter is installed in R: first install the R package `tinytex`, then run the command `tinytex::install_tinytex()` to install this software. 
  - If you are using maize, you don't need to install anything to knit to pdf!
 


-------------------------------------

## Problem 1: USGS Rake data
Consider the rake data used in the day 23 [quasi-binomial worksheet](https://kstclair.github.io/stat-230/Rhandouts/day23_QuasiBinom.pdf). 

```{r}
RakeData <- read.csv("http://people.carleton.edu/~kstclair/data/RakeData.csv")
head(RakeData)
```

### (1a) Fit the regular (no quasi-) binomial model that was fit in the day 23 quasi-binomial worksheet: the binomial regression of $y$ on log of biomass, site substrate and site depth. Plot the deviance residuals against fitted values  for this model  and give the row numbers of the three cases with the most extreme residual deviance values. What are the response values for these cases? Are these cases over- or underestimated? \

*Answer: * \
The cases with the most extreme residual deviance values were row numbers 3, 16, and 25. The response values for each of these cases, respectfully, were .333, .333, and 0. All three cases are over estimated as they are below the 0 line.

```{r}
library(broom)
library(ggResidpanel)
rake_glm <- glm(SiteRake/SiteM ~ log(SiteBiom+1) + SiteSub + SiteDepth, family=binomial, weights=SiteM, data=RakeData)
rake_glm_aug <- augment(rake_glm, type.residual = "deviance")
head(rake_glm_aug)

ggplot(rake_glm_aug, aes(x=.fitted, y=.resid)) + geom_point() + geom_hline(yintercept=0)

#resid_interact(rake_glm, type="deviance")
RakeData[c(3, 16, 25),]
```

### (1b) Look at the Cook's distance measure for the regular binomial model. Which case has the highest value? Determine why these cases have high Cook's distance values. \

*Answer: * \
Case number 26 has the highest value. 26 has a very extreme leverage value (because of its low SiteBiom level and high SiteDepth level) and a relatively high residual value. Cases 25 and 16 also had high Cook's distance values. Case 25 has a lower leverage than 26, but it is still higher than most other cases. 25's residual value, however, is very extreme. Case 16's residual value is also very extreme, but it's leverage is relatively low.

```{r}
library(GGally)
library(dplyr)
plot(rake_glm, which=4)
plot(rake_glm, which=5)
summary(RakeData)
RakeData[c(16, 25, 26),]
```

### (1c) Refit the regular binomial model without the *three cases from part (a)*. Run the goodness-of-fit test. Explain why  the results of this test change compared to the results of the GOF test with all cases (done in the day 22 markdown)? \

*Answer: * \

Our p-value is slightly larger than the results of the original GOF test. Our p-value was 0.743699 while the original p-value was 0.7397009. This means that the probability of getting residual deviance values larger than the observed value is 0.743699. By getting rid of the three cases, our p-value increases. This indicates that it is more likely to get our observed residual deviance, so the logistic model fits adequately.

```{r}
RakeDataNoOutliers <- RakeData[-c(3, 16, 25), ]
rake_glm_noO <- glm(SiteRake/SiteM ~ log(SiteBiom+1) + SiteSub + SiteDepth, family=binomial, weights=SiteM, data=RakeDataNoOutliers)
rake_glm_noO_aug <- augment(rake_glm_noO, type.residual = "deviance")
summary(rake_glm_noO)
1-pchisq(15.557, df=20)
```

### (1d) Refit the regular binomial model without the *one case from part (b)* but including the three cases from part (c). Run the goodness-of-fit test. Explain why the GOF test from (c) suggests that the model is adequate (with the 3 unusual residual cases removed) but the test in (d) is not adequate (with the highest Cook's distance case removed). \

*Answer: * \

The p-value when taking out the case with the highest Cook's distance is 0.002743627. This p-value indicates that the logistic model is not adequate. By getting rid of the three cases with the unusual residual values, the logistic model is still adequate. But when we get rid of the case with the highest Cook's distance, the logistic model is no longer adequate, because it's very unlikely that we would have gotten our observed residual deviance value. This can be explained by the fact that the case with the high Cook's distance was very influential on the data.

```{r}
RakeDataNoCooks <- RakeData[-c(26), ]
rake_glm_noO <- glm(SiteRake/SiteM ~ log(SiteBiom+1) + SiteSub + SiteDepth, family=binomial, weights=SiteM, data=RakeDataNoCooks)
summary(rake_glm_noO)
1-pchisq(44.886, df=22)
```

----------------------------------------


## Problem 2: Galapagos: ch. 22 exercise 18
The data set is `ex1220`. **In addition** to parts (a)-(c):

- use a **deviance residual plot** to verify your GOF conclusions in part (a) (remember that most residuals are between $+/- 2$ if the model fits well) 

- and use a **quasi-Poisson** model for (b)-(c) if your GOF test suggests that it is needed. \

(a) Fit the model with log area, log elevation, log of distance from nearest island, and log area of nearest island as explanatory variables; and then check for extra- Poisson variation. \

*Answer: * \

It's possible that the events in this example are not completely independent. If there is one species on an island, it may be more likely that another type of species also exists on that same island. Likewise, if an island lacks a certain species, it is less likely that some other species may be on the island. So there is extra-Poisson variation. Our deviance residual plot supports our conclusion that there is extra Poisson variation because there are a handful of cases that have residuals greater than 2.

```{r}
library(Sleuth3)
galap <- ex1220
head(galap)
galap_glm <- glm(Total ~ log(Area) + log(Elev) + log(DistNear) + log(AreaNear), family = poisson, data = galap)

galap_glm_aug <- augment(galap_glm, type.residual = "deviance")
ggplot(galap_glm_aug, aes(x = Total, y = .resid)) +
  geom_point()
```

(b) Use backward elimination to eliminate insignificant explanatory variables. \

*Answer: * \
First let's eliminate log(Elev). This is the least significant term in the model. After removing log(Elev), log(DistNear) is still insignificant so we can remove it.

```{r}
galap_glm_small <- glm(Total ~ log(Area)+ log(AreaNear), family = quasipoisson, data = galap)
summary(galap_glm_small)

galap_glm_small_aug <- augment(galap_glm_small, type.residual = "deviance")
ggplot(galap_glm_small_aug, aes(x = Total, y = .resid)) +
  geom_point()
```

(c) Describe the effects of the remaining explanatory variables. \

*Answer: * \
The effect of each of the remaining explanatory variables is statistically significant. An increase of 1 unit in log(Area) is associated with an estimated increase of 0.37835 in Total. An increase of one unit of log(AreaNear) is associated with an estimated decrease of 0.09818 in Total.

```{r}

```

-----------------------------------------


## Problem 3: El Nino and Hurricanse: ch. 22 exercise 21 
Data is `ex1028`. In addition to answering the questions for this exercise, add the following: 

- For both models (a) and (b), interpret the effect of the El Nino temperature on the response as it changes from cold to neutral and cold to warm and explain whether these effects are significant. Be careful to use the `ElNino` variable in data set `ex1028` rather than `Temperature`. 

- You should also recode the `WestAfrica` variable (0=dry and 1=wet) to make it a factor variable with wet/dry levels. 

*Answer: * \
(a) The effect of El Nino temp on storms as the temp changes from cold to neutral is an estimated decrease of 0.04463 storms. This effect, however, is not statistically significant.

(b) The effect of El Nino temp on storms as the temp changes from cold to warm is an estimated decrease of 0.46206 storms. This effect is statistically significant.

```{r}
nino <- ex1028
head(nino)
nino$WestAfrica <- recode_factor(nino$WestAfrica, `1`="wet", `0`="dry")
nino_storms_glm <- glm(Storms ~ ElNino + WestAfrica, family = poisson, data=nino)
nino_hurr_glm <- glm(Hurricanes ~ ElNino + WestAfrica, family = poisson, data=nino)
summary(nino_storms_glm)
summary(nino_hurr_glm)

```







